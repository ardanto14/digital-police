{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "Copy of FIX2 of Copy of digital-police train",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayJBUsnhy-XX"
      },
      "source": [
        "#**DigitalPolice Train Model**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "This notebook contains the steps to build the Machine Learning model to detect anomaly from the surveillance camera videos. \n",
        "\n",
        "The steps are:\n",
        "\n",
        "1. Splitting the extracted Anomaly dataset into training and testing, since from the dataset of [UCF101-Crime](https://www.crcv.ucf.edu/research/real-world-anomaly-detection-in-surveillance-videos/) that we got is already provided the splitted Normal dataset in training and testing.\n",
        "2. Create directory for the training and testing dataset\n",
        "3. Load the dataset into colab for training the model\n",
        "4. Building the model : build custom generator, loss, and accuracy callbacks function, and the model referenced from the [paper](https://openaccess.thecvf.com/content_cvpr_2018/papers/Sultani_Real-World_Anomaly_Detection_CVPR_2018_paper.pdf).\n",
        "5. Analyzing the training results and visualization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2odYZUyOHGZX"
      },
      "source": [
        "##**Connect Colab to the GCP Bucket to obtain the extracted dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pVsUowVZNFA"
      },
      "source": [
        "For the training model, we use the dataset that already preprocessed to extracting the features from the video open dataset using the C3D pre-trained model. The extracted dataset is stored at our GCP Bucket called ucf-fcrime in `/out/` directory, then we obtain the dataset to colab for training. In colab we can see it in `folderOnColab/out/` directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXX4uA6Tw9qt"
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D70Z0BWcw-GO"
      },
      "source": [
        "!echo \"deb http://packages.cloud.google.com/apt gcsfuse-bionic main\" > /etc/apt/sources.list.d/gcsfuse.list\n",
        "!curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\n",
        "!apt -qq update\n",
        "!apt -qq install gcsfuse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QX5cjMl1w-JR"
      },
      "source": [
        "!mkdir folderOnColab\n",
        "!gcsfuse --implicit-dirs ucf-fcrime /content/folderOnColab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUd9FnxmXULe"
      },
      "source": [
        "##**Create directory for the training and testing dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyjMkUCCgzg_"
      },
      "source": [
        "Before we split the anomaly dataset into training and testing, we should create and specify the directory to contain the files or data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbZ_UVOFd2xo"
      },
      "source": [
        "# Define directories\n",
        "import os\n",
        "\n",
        "base_dir = '/content/folderOnColab/out'\n",
        "\n",
        "\n",
        "train_anomaly_dir = os.path.join(base_dir, 'Train-Anomaly-2') # Directory with our training anomaly extracted features\n",
        "train_normal_dir = os.path.join(base_dir, 'Train-Normal') # Directory with our training normal extracted features\n",
        "validation_anomaly_dir = os.path.join(base_dir, 'Test-Anomaly-2') # Directory with our validation anomaly extracted features\n",
        "validation_normal_dir = os.path.join(base_dir, 'Test_Normal')# Directory with our validation normal extracted features\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eul4yKUKXK1x"
      },
      "source": [
        "##**Splitting the dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9I9XWO8Bb7dF"
      },
      "source": [
        "The Normal dataset is already divided into Training and Testing before, yet it is not done for the Anomaly dataset. Therefore, we splitted the Anomaly dataset to training and testing into **80 : 20** ratio by selecting random files from total anomaly data in `Anomaly_data` directory, and separate the 80% to training data and the rest to testing data. The anomaly data contains specific crime category dataset i.e Robbery, Vandalism, Stealing, and Assault as we want to detect these type of anomaly. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YD0ibIKde7Z"
      },
      "source": [
        "First, we define the function to split the dataset into training and testing with 80% for the training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKHbPsWyqWQZ"
      },
      "source": [
        "#split dataset\n",
        "\n",
        "import random\n",
        "\n",
        "train_data_percentage = 0.8    # split the Anomaly dataset into 80:20, 80% = 0.8 for training\n",
        "anomaly_files = os.listdir('/content/folderOnColab/out/Anomaly_data') # specify Anomaly data source directory in GCP Bucket\n",
        "print(len(anomaly_files)) #output the total anomaly data\n",
        "\n",
        "def split_train_test( filenames, percentage):\n",
        "  \n",
        "  total_files = len(filenames) #count all files from the dataset\n",
        "  total_train = int(total_files * train_data_percentage) #count files from the dataset for training\n",
        "  \n",
        "  train_set = []                             #create list for the training data\n",
        "  \n",
        "  while (len(train_set) != total_train):     #keep add files for training until reach 80% of data\n",
        "    files = random.randrange(total_files)    #select random files to train\n",
        "    if files not in train_set:\n",
        "      train_set.append(files)                #add files to the training set\n",
        "\n",
        "  test_set = []                              #create list for the testing data\n",
        "  for i in range(total_files):               #iterate to find files for training from all dataset\n",
        "    if i in train_set:\n",
        "      print(\"skipping \" + str(i))            #skip file that already included in training set to put in testing set\n",
        "      continue\n",
        "    else:\n",
        "      test_set.append(i)                     #add files to the training set\n",
        "    \n",
        "  train_files =[]\n",
        "  for i in train_set:    \n",
        "    train_files.append(filenames[i])\n",
        "   \n",
        "  test_files =[]\n",
        "  for i in test_set:    \n",
        "    test_files.append(filenames[i])\n",
        "  \n",
        "  return (train_files, test_files)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sr5PFeyddwie"
      },
      "source": [
        "Then we do the splitting for the anomaly data while skipping the existing files in the training data to separate the testing data, without containing the same data within the training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDRDzKaDsdmI"
      },
      "source": [
        "anomaly_train_test = (split_train_test (anomaly_files , train_data_percentage)) # splitting the dataset\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fg6ALZBdeXHQ"
      },
      "source": [
        "after separating the dataset we copy the splitted dataset into specific directories for each training and testing anomaly data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rsO9pU8skgk"
      },
      "source": [
        "# Copy Anomaly files from source directory to train directory\n",
        "from shutil import copyfile    \n",
        "\n",
        "base_dir = \"/content/folderOnColab/out/\" #specify directory path\n",
        "\n",
        "# Copy Anomaly train files from source directory to train directory\n",
        "length = len (anomaly_train_test[0])\n",
        "for i in range(length):\n",
        "  source = (\"/content/folderOnColab/out/Anomaly_data/\" + anomaly_train_test[0][i])\n",
        "  destination = (train_anomaly_dir + \"/\" + anomaly_train_test[0][i])\n",
        "  copyfile(source, destination)\n",
        "  print (\"Copying anomaly train files\")\n",
        "  \n",
        "# Copy Anomaly test files from source directory to train directory\n",
        "length = len (anomaly_train_test[1])\n",
        "for i in range(length):\n",
        "  source = (\"/content/folderOnColab/out/Anomaly_data/\" + anomaly_train_test[1][i])\n",
        "  destination = (validation_anomaly_dir + \"/\" + anomaly_train_test[1][i])\n",
        "  copyfile(source, destination)\n",
        "  print (\"Copying anomaly test files\")\n",
        "\n",
        "train_anomaly = os.listdir(train_anomaly_dir) \n",
        "valid_anomaly = os.listdir(validation_anomaly_dir) \n",
        "print(f\"Number of train files with anomaly copied: {len(train_anomaly)}\")  \n",
        "print(f\"Number of test files with anomaly copied: {len(valid_anomaly)}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpMc8yGLX-uo"
      },
      "source": [
        "##**Load the dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLz1xl-mhRCK"
      },
      "source": [
        "Then, we should load the dataset that we want to used into colab to do the training step. First we need to import the necessary modules, then create the function to load the dataset, and last do the loading. To avoid huge use of memory in colab we should clear the session."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsDcKhUK4Ig6"
      },
      "source": [
        "#import necessary modules\n",
        "import numpy as np\n",
        "import random\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZ-k8ne41Jkn"
      },
      "source": [
        "# create function to load the dataset from GCP Bucket to Colab\n",
        "\n",
        "def load_all_dataset(abnormal_path, normal_path):\n",
        "\n",
        "  normal_video_take = len(os.listdir(normal_path))\n",
        "  abnormal_video_take = len(os.listdir(abnormal_path))\n",
        "\n",
        "  normal_folder = os.listdir(normal_path)\n",
        "  abnormal_folder = os.listdir(abnormal_path)\n",
        "\n",
        "  len_normal_folder = len(normal_folder)\n",
        "  len_abnormal_folder = len(abnormal_folder)\n",
        "\n",
        "\n",
        "  normal_list = normal_folder # np.random.choice(normal_folder, size=normal_video_take, replace=False)\n",
        "  abnormal_list = abnormal_folder # np.random.choice(abnormal_folder, size=abnormal_video_take, replace=False)\n",
        "\n",
        "  all_normal = []\n",
        "  all_abnormal = []\n",
        "\n",
        "  for normal_file in normal_list:\n",
        "    try:\n",
        "      with open(os.path.join(normal_path, normal_file), 'r') as normal_f:\n",
        "        print(\"reading \" + normal_file)\n",
        "        normal_words = normal_f.read().split()\n",
        "        normal_num_feat = len(normal_words) / 4096\n",
        "        btch_normal = []\n",
        "        for normal_feat in range(0, int(normal_num_feat)):\n",
        "          normal_feat_row = np.float32(normal_words[normal_feat * 4096:normal_feat * 4096 + 4096])\n",
        "          btch_normal.append(normal_feat_row)\n",
        "        all_normal.append(np.array(btch_normal))\n",
        "\n",
        "    except IsADirectoryError:\n",
        "      print(\"continue because \" + normal_file + \" is a directory\")\n",
        "      pass\n",
        "\n",
        "  for abnormal_file in abnormal_list:\n",
        "    try:\n",
        "      with open(os.path.join(abnormal_path, abnormal_file), 'r') as abnormal_f:\n",
        "        print(\"reading \" + abnormal_file)\n",
        "        abnormal_words = abnormal_f.read().split()\n",
        "        abnormal_num_feat = len(abnormal_words) / 4096\n",
        "        btch_abnormal = []\n",
        "        for abnormal_feat in range(0, int(abnormal_num_feat)):\n",
        "          abnormal_feat_row = np.float32(abnormal_words[abnormal_feat * 4096:abnormal_feat * 4096 + 4096])\n",
        "          btch_abnormal.append(abnormal_feat_row)\n",
        "\n",
        "        all_abnormal.append(np.array(btch_abnormal))\n",
        "\n",
        "    except IsADirectoryError:\n",
        "      print(\"continue because \" + abnormal_file + \" is a directory\")\n",
        "      pass\n",
        "\n",
        "  return np.array(all_normal), np.array(all_abnormal)\n",
        "      \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_ygv0EayUkC"
      },
      "source": [
        "#load the dataset for training\n",
        "\n",
        "train_normal_x, train_abnormal_x = load_all_dataset(train_anomaly_dir, train_normal_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wO8rqyJHJj0S"
      },
      "source": [
        "#load the dataset for testing\n",
        "test_normal_x, test_abnormal_x = load_all_dataset(validation_anomaly_dir, validation_normal_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRsE5z1B0Vj1"
      },
      "source": [
        "tf.keras.backend.clear_session()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1OIZWoxYG2f"
      },
      "source": [
        "##**Building the model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vcq_daZEh_vY"
      },
      "source": [
        "After the dataset is ready, then we build the model. In this model we use custom generator, loss function, and accuracy callback referencing to the resource paper."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDcgIF_riapU"
      },
      "source": [
        "First, we create the custom generator and specify each for training and testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTq_0Jvbr34c"
      },
      "source": [
        "#create custom generator\n",
        "\n",
        "def generator(normal_x, abnormal_x, batchsize):\n",
        "  n_exp= int(batchsize/2)\n",
        "  \n",
        "  \n",
        "  while True:\n",
        "    normal_x_indices = np.arange(normal_x.shape[0])\n",
        "    abnormal_x_indices = np.arange(abnormal_x.shape[0])\n",
        "\n",
        "    normal_get = np.random.choice(normal_x_indices, n_exp, replace=False)\n",
        "    abnormal_get = np.random.choice(abnormal_x_indices, n_exp, replace=False)\n",
        "\n",
        "\n",
        "    Abnor_list_iter = abnormal_x[abnormal_get]\n",
        "    Norm_list_iter = normal_x[normal_get]\n",
        "    \n",
        "\n",
        "    AllFeatures = []\n",
        "    # print(\"Loading Anomaly videos Features...\")\n",
        "\n",
        "    Video_count=-1\n",
        "    for normal_video in Norm_list_iter:\n",
        "      Video_count=Video_count+1\n",
        "      count = -1;\n",
        "      VideoFeatures = []\n",
        "      for feat in normal_video:\n",
        "        count = count + 1\n",
        "        if count == 0:\n",
        "          VideoFeatures = feat\n",
        "        if count > 0:\n",
        "          VideoFeatures = np.vstack((VideoFeatures, feat))\n",
        "\n",
        "      if Video_count == 0:\n",
        "        AllFeatures = VideoFeatures\n",
        "      if Video_count > 0:\n",
        "        AllFeatures = np.vstack((AllFeatures, VideoFeatures))\n",
        "\n",
        "    # print(\" Abnormal Features  loaded\")\n",
        "    \n",
        "    # print(\"Loading Normal videos...\")\n",
        "    # print(AllFeatures.shape)\n",
        "    for abnormal_video in Abnor_list_iter:\n",
        "      Video_count=Video_count+1\n",
        "      # print(\"[{}] loading vide {}\".format(Video_count, VideoPath))\n",
        "      \n",
        "      count = -1;\n",
        "      VideoFeatures = []\n",
        "      for feat in abnormal_video:\n",
        "        count = count + 1\n",
        "        if count == 0:\n",
        "          VideoFeatures = feat\n",
        "        if count > 0:\n",
        "          VideoFeatures = np.vstack((VideoFeatures, feat))\n",
        "\n",
        "      AllFeatures = np.vstack((AllFeatures, VideoFeatures))\n",
        "\n",
        "    # print(\"Features  loaded\")\n",
        "\n",
        "    AllLabels = np.zeros(32*batchsize, dtype='float32')\n",
        "    th_loop1=n_exp*32\n",
        "    th_loop2=n_exp*32-1\n",
        "\n",
        "    for iv in range(0, 32*batchsize):\n",
        "      if iv< th_loop1:\n",
        "        AllLabels[iv] = 0.0\n",
        "      if iv > th_loop2:\n",
        "        AllLabels[iv] = 1.0\n",
        "\n",
        "    # print(np.array(AllFeatures).shape)\n",
        "    # print(np.array(AllLabels).shape)\n",
        "\n",
        "    yield  np.array(AllFeatures), np.array(AllLabels)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UK9LCu36d7CM"
      },
      "source": [
        "#specify the generator for training and testing with batch size = 10\n",
        "\n",
        "train_generator = generator(train_normal_x, train_abnormal_x, 10)\n",
        "validation_generator = generator(test_normal_x, test_abnormal_x, 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OP1IMlQyiuKS"
      },
      "source": [
        "Then, create the Accuracy (ACC) calback function "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8nAPKnk-tSe"
      },
      "source": [
        "# create accuracy callback function\n",
        "\n",
        "class AccCallback(Callback):\n",
        "  def __init__(self,normal_x, abnormal_x):\n",
        "    self.normal_x = normal_x\n",
        "    self.abnormal_x = abnormal_x\n",
        "\n",
        "\n",
        "  def on_train_begin(self, logs={}):\n",
        "    return\n",
        "\n",
        "  def on_train_end(self, logs={}):\n",
        "    return\n",
        "\n",
        "  def on_epoch_begin(self, epoch, logs={}):\n",
        "    return\n",
        "\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "    if epoch % 100 == 0:\n",
        "      tp = 0\n",
        "      tn = 0\n",
        "      fp = 0\n",
        "      fn = 0\n",
        "\n",
        "      for vid in self.abnormal_x:\n",
        "        pr = self.model.predict(vid)\n",
        "\n",
        "        pr = pr > 0.5\n",
        "\n",
        "        out = True in pr\n",
        "\n",
        "        if out:\n",
        "          tp += 1\n",
        "        else:\n",
        "          fn += 1\n",
        "\n",
        "      for vid in self.normal_x:\n",
        "        pr = model.predict(vid)\n",
        "\n",
        "        pr = pr > 0.5\n",
        "\n",
        "        out = True in pr\n",
        "\n",
        "        if out:\n",
        "          fp += 1\n",
        "        else:\n",
        "          tn += 1\n",
        "\n",
        "      print('tp = {}, tn = {}, fp = {}, fn = {}, precision = {}, recall = {}, acc = {}'.format(tp, tn, fp, fn, tp / (tp + fp), tp / (tp + fn), (tp + tn) / (tp + tn + fp + fn)))\n",
        "    return\n",
        "\n",
        "  def on_batch_begin(self, batch, logs={}):\n",
        "    return\n",
        "\n",
        "  def on_batch_end(self, batch, logs={}):\n",
        "    return\n",
        "\n",
        "acc = AccCallback(test_normal_x, test_abnormal_x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4eY9_Ki0i9un"
      },
      "source": [
        "after that, create the custom loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4dFEhpVnOIt"
      },
      "source": [
        "#create custom loss function\n",
        "\n",
        "@tf.function\n",
        "def custom_objective(y_true, y_pred):\n",
        "    \n",
        "  y_true = tf.reshape(y_true, [-1])\n",
        "  y_pred = tf.reshape(y_pred, [-1])\n",
        "\n",
        "  # print(y_true)\n",
        "  # print(y_pred)\n",
        "  \n",
        "  n_seg = 32\n",
        "  nvid = 10\n",
        "  n_exp = nvid / 2\n",
        "  n_exp = int(n_exp)\n",
        "  Num_d=32*nvid\n",
        "\n",
        "  sub_max = tf.ones_like(y_pred)\n",
        "  sub_sum_labels = tf.ones_like(y_true)\n",
        "  sub_sum_l1= tf.ones_like(y_true) \n",
        "  sub_l2 = tf.ones_like(y_true)\n",
        "\n",
        "  for ii in range(0, nvid, 1):\n",
        "    \n",
        "    mm = y_true[ii * n_seg:ii * n_seg + n_seg]\n",
        "    # print(tf.stack([tf.math.reduce_sum(mm)]))\n",
        "    sub_sum_labels = tf.concat([sub_sum_labels, tf.stack([tf.math.reduce_sum(mm)])], 0)\n",
        "\n",
        "    Feat_Score = y_pred[ii * n_seg:ii * n_seg + n_seg]\n",
        "    sub_max = tf.concat([sub_max, tf.stack([tf.math.reduce_max(Feat_Score)])], 0)  \n",
        "    sub_sum_l1 = tf.concat([sub_sum_l1, tf.stack([tf.math.reduce_sum(Feat_Score)])], 0)\n",
        "\n",
        "    z1 = tf.ones_like(Feat_Score)\n",
        "    z2 = tf.concat([z1, Feat_Score], 0)\n",
        "    z3 = tf.concat([Feat_Score, z1], 0)\n",
        "    z_22 = z2[31:]\n",
        "    z_44 = z3[:33]\n",
        "    z = z_22 - z_44\n",
        "    z = z[1:32]\n",
        "    z = tf.math.reduce_sum(tf.math.square(z))\n",
        "    sub_l2 = tf.concat([sub_l2, tf.stack([z])], 0)\n",
        "\n",
        "\n",
        "  sub_score = sub_max[Num_d:]\n",
        "  F_labels = sub_sum_labels[Num_d:]\n",
        "  \n",
        "\n",
        "  sub_sum_l1 = sub_sum_l1[Num_d:]\n",
        "  sub_sum_l1 = sub_sum_l1[:n_exp]\n",
        "  sub_l2 = sub_l2[Num_d:]\n",
        "  sub_l2 = sub_l2[:n_exp]\n",
        "\n",
        "  indx_nor = tf.where(tf.math.equal(F_labels, 0))# tf.experimental.numpy.nonzero(tf.cond(tf.math.equal(F_labels, 32), lambda: 1, lambda: 0))[0]\n",
        "  indx_abn = tf.where(tf.math.equal(F_labels, 32))# tf.experimental.numpy.nonzero(tf.cond(tf.math.equal(F_labels, 0), lambda: 1, lambda: 0))[0]\n",
        "\n",
        "  n_Nor=n_exp\n",
        "\n",
        "  Sub_Nor = tf.gather_nd(sub_score, indx_nor)\n",
        "  Sub_Abn = tf.gather_nd(sub_score, indx_abn)\n",
        "\n",
        "  z = tf.ones_like(y_true)\n",
        "  for ii in range(0, n_Nor, 1):\n",
        "    # print(Sub_Abn)\n",
        "    # print(Sub_Nor)\n",
        "    sub_z = tf.math.maximum(1 - Sub_Abn[ii] + Sub_Nor[ii], 0)\n",
        "    z = tf.concat([z, tf.stack([tf.math.reduce_sum(sub_z)])], 0)\n",
        "\n",
        "  z = z[Num_d:]\n",
        "  z = tf.math.reduce_mean(z, axis=-1) +  0.00008*tf.math.reduce_sum(sub_sum_l1) + 0.00008*tf.math.reduce_sum(sub_l2)\n",
        "  # print(z)\n",
        "  return z"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKS7Vjp1jKFB"
      },
      "source": [
        "Lasts, build the CNN model that referenced to the resources paper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZ1jP1oEv5ru"
      },
      "source": [
        "#building the CNN model with input shape 4096 as a result from the extracted features using C3D\n",
        "#use 0.6 dropout between the layers\n",
        "#output one to determine anomaly (1) or normal (0)\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "                             tf.keras.layers.Input(shape=(4096,)),\n",
        "                             tf.keras.layers.Dropout(0.6),\n",
        "                             tf.keras.layers.Dense(512, kernel_initializer='glorot_normal', kernel_regularizer=tf.keras.regularizers.L2(0.001), activation='relu'),\n",
        "                             tf.keras.layers.Dropout(0.6),\n",
        "                             tf.keras.layers.Dense(32, kernel_initializer='glorot_normal', kernel_regularizer=tf.keras.regularizers.L2(0.001)),\n",
        "                             tf.keras.layers.Dropout(0.6),\n",
        "                             tf.keras.layers.Dense(1, kernel_initializer='glorot_normal', kernel_regularizer=tf.keras.regularizers.L2(0.001), activation='sigmoid'),\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUtuxzHZjVNu"
      },
      "source": [
        "To compile the model we use Adagrad optimizer with learning rate 0.1 and use the custom loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "theuMG36xck2"
      },
      "source": [
        "#compile the model with Adagrad optimizer and custom loss function\n",
        "model.compile(optimizer=tf.keras.optimizers.Adagrad(0.01, epsilon=1e-08), loss=custom_objective)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "on_LZY3Ajk4m"
      },
      "source": [
        "Train the model in 500 epochs to avoid overfitting. For analyses purposes we specify the model into history variable to plot it in graph visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6wq3PlcZQ6J"
      },
      "source": [
        "history = model.fit(train_generator, validation_data=validation_generator, epochs=500, steps_per_epoch=1, validation_steps=1, callbacks=[acc])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvYLIs8kkMzh"
      },
      "source": [
        "###**save the model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ulu7gI8mkRbq"
      },
      "source": [
        "After the model is done training, we save the model for deployment purpose. \n",
        "\n",
        "Download it from the colab to store it in cloud for further deployment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgfS2zoj89LG"
      },
      "source": [
        "model.save('model-v2.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myavd945tq4x"
      },
      "source": [
        "tf.saved_model.save(model, 'wuoot')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCRp0aZnuH0x"
      },
      "source": [
        "!zip -r /content/file.zip /content/wuoot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_cFiufCO0FS"
      },
      "source": [
        "##**Analyzing the results**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_slbKuIkk5dR"
      },
      "source": [
        "To make sure we have the proper model, we do analyze the results through loss graph, and knowing the ROC, precision, f1, recall and False Alarm Rate values to decide whether it is already good or not to be used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gufp-Fw5lnLC"
      },
      "source": [
        "###**Loss graph**\n",
        "first, we plot the `loss` and `val_loss` from the model to decide best number for the epochs to avoid overfitting model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kw6i9-EpX9Ok"
      },
      "source": [
        "#plot the loss and val_loss per epochs to a graph to know the proper epochs to avoid overfitting\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(loss))\n",
        "\n",
        "plt.plot(epochs, loss, 'r', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend(loc=0)\n",
        "plt.figure()\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdbpmDqcmIKx"
      },
      "source": [
        "load weights from the saved model to further results analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HM3OYDUowZgQ"
      },
      "source": [
        "#load model weights from the saved model\n",
        "\n",
        "model.load_weights('model-v2.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u28RMgOymWYA"
      },
      "source": [
        "###**Confusion matrix**\n",
        "Analyze the confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOxi69VpWTN_"
      },
      "source": [
        "#analyze the confusion matrix from the model, only count the prediction score that have > 0.5 values as the threshold\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "new_pred = prediction > 0.5\n",
        "\n",
        "a = confusion_matrix(yy, new_pred)\n",
        "\n",
        "tn, fp, fn, tp = a.ravel()\n",
        "print(a)\n",
        "print(\"tn = {}, fp = {}, fn = {}, tp = {}\".format(tn, fp, fn, tp))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVDr9QvonH2h"
      },
      "source": [
        "###**Precision, Recall, F1, and False Alarm Rate**\n",
        "analyze the precision, False Alarm Rate (from false positive rate), recall and f1 from the training result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tKYOUFrYgod"
      },
      "source": [
        "precision = tp / (tp + fp)\n",
        "FAR = fp / (fp + tn)\n",
        "recall = tp / (tp + fn)\n",
        "f1 = 2 * precision * recall / (precision + recall)\n",
        "\n",
        "print(\"precision = {}, FAR (False Alarm rate = {}, recall = {}, f1 = {}\".format(precision, FAR, recall, f1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StpSRZESZYCt"
      },
      "source": [
        "#analyze the predicted result from test abnormal or anomaly videos, indicates True for the anomaly with >0.5 values as it is nearer to the anomaly values (1)\n",
        "\n",
        "true = 0\n",
        "false = 0\n",
        "\n",
        "for vid in test_abnormal_x:\n",
        "  pr = model.predict(vid)\n",
        "\n",
        "  pr = pr > 0.5\n",
        "\n",
        "  out = True in pr\n",
        "\n",
        "  if out:\n",
        "    true += 1\n",
        "  else:\n",
        "    false += 1\n",
        "\n",
        "  print(out)\n",
        "\n",
        "print(true / (true + false))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFzUtoh99eOJ"
      },
      "source": [
        "#analyze the predicted result from test normal videos, indicates True for the anomaly with >0.5 values as it is nearer to the anomaly values (1)\n",
        "\n",
        "true = 0\n",
        "false = 0\n",
        "\n",
        "for vid in test_normal_x:\n",
        "  pr = model.predict(vid)\n",
        "\n",
        "  pr = pr > 0.5\n",
        "\n",
        "  out = True in pr\n",
        "\n",
        "  if out:\n",
        "    false += 1\n",
        "  else:\n",
        "    true += 1\n",
        "\n",
        "  print(out)\n",
        "\n",
        "print(true / (true + false))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ii4yW-vR_szN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3Hl-yNb_eTn"
      },
      "source": [
        "##**Test Ground**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kb5niNzHBEpG"
      },
      "source": [
        "This section is for testing the model with test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBlhX92iBt1b"
      },
      "source": [
        "#import necessary module\n",
        "\n",
        "import re\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQVNoUlUCB8X"
      },
      "source": [
        "We import the C3D pretrained model referenced to `adamcasson` in [github](https://github.com/ardanto14/digital-police/blob/main/C3D/C3D.py), to do the testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_O4cDoM0BvnN"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"C3D model for Keras\n",
        "# Reference:\n",
        "- [Learning Spatiotemporal Features with 3D Convolutional Networks](https://arxiv.org/abs/1412.0767)\n",
        "Based on code from @albertomontesg\n",
        "\"\"\"\n",
        "\n",
        "import skvideo.io\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.utils import get_file\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv3D, MaxPooling3D, ZeroPadding3D\n",
        "\n",
        "WEIGHTS_PATH = 'https://github.com/adamcasson/c3d/releases/download/v0.1/sports1M_weights_tf.h5'\n",
        "\n",
        "def C3D(weights='sports1M'):\n",
        "    \"\"\"Instantiates a C3D Kerasl model\n",
        "    \n",
        "    Keyword arguments:\n",
        "    weights -- weights to load into model. (default is sports1M)\n",
        "    \n",
        "    Returns:\n",
        "    A Keras model.\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    if weights not in {'sports1M', None}:\n",
        "        raise ValueError('weights should be either be sports1M or None')\n",
        "    \n",
        "    if K.image_data_format() == 'channels_last':\n",
        "        shape = (16,112,112,3)\n",
        "    else:\n",
        "        shape = (3,16,112,112)\n",
        "        \n",
        "    model = Sequential()\n",
        "    model.add(Conv3D(64, 3, activation='relu', padding='same', name='conv1', input_shape=shape))\n",
        "    model.add(MaxPooling3D(pool_size=(1,2,2), strides=(1,2,2), padding='same', name='pool1'))\n",
        "    \n",
        "    model.add(Conv3D(128, 3, activation='relu', padding='same', name='conv2'))\n",
        "    model.add(MaxPooling3D(pool_size=(2,2,2), strides=(2,2,2), padding='valid', name='pool2'))\n",
        "    \n",
        "    model.add(Conv3D(256, 3, activation='relu', padding='same', name='conv3a'))\n",
        "    model.add(Conv3D(256, 3, activation='relu', padding='same', name='conv3b'))\n",
        "    model.add(MaxPooling3D(pool_size=(2,2,2), strides=(2,2,2), padding='valid', name='pool3'))\n",
        "    \n",
        "    model.add(Conv3D(512, 3, activation='relu', padding='same', name='conv4a'))\n",
        "    model.add(Conv3D(512, 3, activation='relu', padding='same', name='conv4b'))\n",
        "    model.add(MaxPooling3D(pool_size=(2,2,2), strides=(2,2,2), padding='valid', name='pool4'))\n",
        "    \n",
        "    model.add(Conv3D(512, 3, activation='relu', padding='same', name='conv5a'))\n",
        "    model.add(Conv3D(512, 3, activation='relu', padding='same', name='conv5b'))\n",
        "    model.add(ZeroPadding3D(padding=(0,1,1)))\n",
        "    model.add(MaxPooling3D(pool_size=(2,2,2), strides=(2,2,2), padding='valid', name='pool5'))\n",
        "    \n",
        "    model.add(Flatten())\n",
        "    \n",
        "    model.add(Dense(4096, activation='relu', name='fc6'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(4096, activation='relu', name='fc7'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(487, activation='softmax', name='fc8'))\n",
        "\n",
        "    if weights == 'sports1M':\n",
        "        weights_path = get_file('sports1M_weights_tf.h5',\n",
        "                                WEIGHTS_PATH,\n",
        "                                cache_subdir='models',\n",
        "                                md5_hash='b7a93b2f9156ccbebe3ca24b41fc5402')\n",
        "        \n",
        "        model.load_weights(weights_path)\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYJtUd78B0dT"
      },
      "source": [
        "#copy model from colab\n",
        "!cp /content/model-v2.h5 classify_weights_tf.h5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XznAJw9hDH2f"
      },
      "source": [
        "import the utils.py to trained the C3D, we already put the code in this [github](https://github.com/ardanto14/digital-police/blob/main/C3D/utils.py)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Up1o2FhUB04Z"
      },
      "source": [
        "import numpy as np\n",
        "from scipy.misc import imresize\n",
        "from tensorflow.keras.utils import get_file\n",
        "\n",
        "\n",
        "C3D_MEAN_PATH = 'https://github.com/adamcasson/c3d/releases/download/v0.1/c3d_mean.npy'\n",
        "SPORTS1M_CLASSES_PATH = 'https://github.com/adamcasson/c3d/releases/download/v0.1/sports1M_classes.txt'\n",
        "\n",
        "def preprocess_input(video):\n",
        "    \"\"\"Resize and subtract mean from video input\n",
        "    \n",
        "    Keyword arguments:\n",
        "    video -- video frames to preprocess. Expected shape \n",
        "        (frames, rows, columns, channels). If the input has more than 16 frames\n",
        "        then only 16 evenly samples frames will be selected to process.\n",
        "    \n",
        "    Returns:\n",
        "    A numpy array.\n",
        "    \n",
        "    \"\"\"\n",
        "    intervals = np.ceil(np.linspace(0, video.shape[0]-1, 16)).astype(int)\n",
        "    frames = video[intervals]\n",
        "    \n",
        "    # Reshape to 128x171\n",
        "    reshape_frames = np.zeros((frames.shape[0], 128, 171, frames.shape[3]))\n",
        "    for i, img in enumerate(frames):\n",
        "        img = imresize(img, (128,171), 'bicubic')\n",
        "        reshape_frames[i,:,:,:] = img\n",
        "        \n",
        "    mean_path = get_file('c3d_mean.npy',\n",
        "                         C3D_MEAN_PATH,\n",
        "                         cache_subdir='models',\n",
        "                         md5_hash='08a07d9761e76097985124d9e8b2fe34')\n",
        "    \n",
        "    # Subtract mean\n",
        "    mean = np.load(mean_path)\n",
        "    reshape_frames -= mean\n",
        "    # Crop to 112x112\n",
        "    reshape_frames = reshape_frames[:,8:120,30:142,:]\n",
        "    # Add extra dimension for samples\n",
        "    reshape_frames = np.expand_dims(reshape_frames, axis=0)\n",
        "    \n",
        "    return reshape_frames\n",
        "\n",
        "def decode_predictions(preds):\n",
        "    \"\"\"Returns class label and confidence of top predicted answer\n",
        "    \n",
        "    Keyword arguments:\n",
        "    preds -- numpy array of class probability\n",
        "    \n",
        "    Returns:\n",
        "    A list of tuples.\n",
        "    \n",
        "    \"\"\"\n",
        "    class_pred = []\n",
        "    for x in range(preds.shape[0]):\n",
        "        class_pred.append(np.argmax(preds[x]))\n",
        "    \n",
        "    labels_path = get_file('sports1M_classes.txt',\n",
        "                           SPORTS1M_CLASSES_PATH,\n",
        "                           cache_subdir='models',\n",
        "                           md5_hash='c102dd9508f3aa8e360494a8a0468ad9')\n",
        "    \n",
        "    with open(labels_path, 'r') as f:\n",
        "        labels = [lines.strip() for lines in f]\n",
        "        \n",
        "    decoded = [(labels[x],preds[i,x]) for i,x in enumerate(class_pred)]\n",
        "    \n",
        "    return decoded"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6fFYc6w_h7x"
      },
      "source": [
        "endfile = []\n",
        "\n",
        "for file in os.listdir('/content/folderOnColab/out/Test-Anomaly'):\n",
        "  result = re.search(r\"([a-zA-Z]*)(\\d*)_\", file)\n",
        "  path = \"/content/folderOnColab/\" + result.groups()[0] + '/'\n",
        "  filename = file.split('.')[0] + '.mp4'\n",
        "  endfile.append(path + filename)\n",
        "\n",
        "print(endfile)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHmdZC_vAT6T"
      },
      "source": [
        "endfile2 = []\n",
        "\n",
        "for file in os.listdir('/content/folderOnColab/out/Test_Normal'):\n",
        "  result = re.search(r\"([a-zA-Z]*)(\\d*)_\", file)\n",
        "  path = \"/content/folderOnColab/\" + 'Testing-Normal-Videos' + '/'\n",
        "  \n",
        "  filename = file.split('.')[0] + '.mp4'\n",
        "\n",
        "  \n",
        "  endfile2.append(path + filename)\n",
        "\n",
        "print(endfile2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3w_2rfi9AXwZ"
      },
      "source": [
        "from tensorflow.keras import Model\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "\n",
        "base_model = C3D(weights='sports1M')\n",
        "feature_extractor = Model(inputs=base_model.input, outputs=base_model.get_layer('fc6').output)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "prediction_model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(4096,)),\n",
        "    tf.keras.layers.Dropout(0.6),\n",
        "    tf.keras.layers.Dense(512, kernel_initializer='glorot_normal', kernel_regularizer=tf.keras.regularizers.L2(0.001), activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.6),\n",
        "    tf.keras.layers.Dense(32, kernel_initializer='glorot_normal', kernel_regularizer=tf.keras.regularizers.L2(0.001)),\n",
        "    tf.keras.layers.Dropout(0.6),\n",
        "    tf.keras.layers.Dense(1, kernel_initializer='glorot_normal', kernel_regularizer=tf.keras.regularizers.L2(0.001), activation='sigmoid'),\n",
        "])\n",
        "\n",
        "prediction_model.load_weights('classify_weights_tf.h5')\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    feature_extractor,\n",
        "    tf.keras.layers.Flatten(),\n",
        "    prediction_model\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDjMiHdDDcVu"
      },
      "source": [
        "after importing the pretrained model we could do the testing for anomaly and normal testing dataset implementing our model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-uIvlPCAeZo"
      },
      "source": [
        "tp = 0\n",
        "tn = 0\n",
        "fn = 0\n",
        "fp = 0\n",
        "\n",
        "for fil in endfile:\n",
        "  print('processing video {}'.format(fil))\n",
        "  cap = cv2.VideoCapture(fil)\n",
        "\n",
        "  batch_size = 32\n",
        "  ln = 0\n",
        "  ln_btch = 0\n",
        "  all_batch = []\n",
        "  all_frame = []\n",
        "  is_anomaly = False\n",
        "  while True:\n",
        "      \n",
        "      ret, frame = cap.read()\n",
        "      if not ret:\n",
        "          break\n",
        "\n",
        "      all_frame.append(frame)\n",
        "      ln += 1\n",
        "\n",
        "      if ln == 16:\n",
        "          x = preprocess_input(np.array(all_frame))\n",
        "          x = np.squeeze(x)\n",
        "          \n",
        "          all_batch.append(x)\n",
        "          ln_btch += 1\n",
        "\n",
        "          if ln_btch == batch_size:\n",
        "              prediction = model.predict(np.array(all_batch))\n",
        "\n",
        "              prediction = prediction > 0.5\n",
        "\n",
        "              if prediction.any():\n",
        "                  is_anomaly = True\n",
        "\n",
        "              all_batch = []\n",
        "              ln_btch = 0\n",
        "          \n",
        "          \n",
        "          \n",
        "          all_frame = []\n",
        "          ln = 0\n",
        "\n",
        "  if ln_btch > 0:\n",
        "      prediction = model.predict(np.array(all_batch))\n",
        "\n",
        "      prediction = prediction > 0.5\n",
        "\n",
        "      if prediction.any():\n",
        "          is_anomaly = True\n",
        "\n",
        "      all_batch = []\n",
        "      ln_btch = 0\n",
        "\n",
        "  if is_anomaly:\n",
        "    tp += 1\n",
        "  else:\n",
        "    fn += 1\n",
        "\n",
        "  cap.release()\n",
        "\n",
        "tn = 0\n",
        "fp = 0\n",
        "\n",
        "for fil in endfile2:\n",
        "  print('processing video {}'.format(fil))\n",
        "  cap = cv2.VideoCapture(fil)\n",
        "\n",
        "  batch_size = 32\n",
        "  ln = 0\n",
        "  ln_btch = 0\n",
        "  all_batch = []\n",
        "  all_frame = []\n",
        "  is_anomaly = False\n",
        "  while True:\n",
        "      \n",
        "      ret, frame = cap.read()\n",
        "      if not ret:\n",
        "          break\n",
        "\n",
        "      all_frame.append(frame)\n",
        "      ln += 1\n",
        "\n",
        "      if ln == 16:\n",
        "          x = preprocess_input(np.array(all_frame))\n",
        "          x = np.squeeze(x)\n",
        "          \n",
        "          all_batch.append(x)\n",
        "          ln_btch += 1\n",
        "\n",
        "          if ln_btch == batch_size:\n",
        "              prediction = model.predict(np.array(all_batch))\n",
        "\n",
        "              prediction = prediction > 0.5\n",
        "\n",
        "              if prediction.any():\n",
        "                  is_anomaly = True\n",
        "\n",
        "              all_batch = []\n",
        "              ln_btch = 0\n",
        "          \n",
        "          \n",
        "          \n",
        "          all_frame = []\n",
        "          ln = 0\n",
        "\n",
        "  if ln_btch > 0:\n",
        "      prediction = model.predict(np.array(all_batch))\n",
        "\n",
        "      prediction = prediction > 0.5\n",
        "\n",
        "      if prediction.any():\n",
        "          is_anomaly = True\n",
        "\n",
        "      all_batch = []\n",
        "      ln_btch = 0\n",
        "\n",
        "  if is_anomaly:\n",
        "    fp += 1\n",
        "  else:\n",
        "    tn += 1\n",
        "\n",
        "  cap.release()\n",
        "\n",
        "precision = tp + (tp / fp)\n",
        "recall = tp + (tp / fn)\n",
        "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
        "FAR = fp / (fp + tn)\n",
        "print(\"tp = {}, tn = {}, fp = {}, fn = {}, precision = {}, recall = {}, accuracy = {}, FAR = {}\".format(tp, tn, fp, fn, precision, recall, accuracy, FAR))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}